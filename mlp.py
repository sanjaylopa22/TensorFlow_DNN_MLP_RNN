# -*- coding: utf-8 -*-
"""Mlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_2-6IruY9Pz3yKAQ5rHorD36Zupzy3vY

Importing modules
"""

##loading module
import numpy as np
import pandas as pd
import cv2
#from keras.models import load_model
import keras.backend as k1
from sklearn.model_selection import train_test_split
from skimage.io import imread,imshow
from skimage.transform import resize
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Flatten,Dropout,GlobalAveragePooling2D,Convolution2D,AveragePooling2D,MaxPooling2D
from tensorflow.keras.layers import BatchNormalization

from tensorflow.keras.models import Model
from tensorflow.keras.applications import DenseNet121,DenseNet169

#from keras.applications.densenet import preprocess_input
from tensorflow.keras.regularizers import l2
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from glob import glob
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

"""Data read"""

path="augmented-image/"
l=glob(path+"*")
s=len(glob(path+"*"))
print(l)

"""Data Extraction"""

'''
Extract features from images '''
path="augmented-image/"
l=glob(path+"*")
s=len(glob(path+"*"))
extract=[]
name_items=[]
for i in l:
    name_items.append(i[16:])
print(name_items)

flat_data_arr=[] #input array
target_arr=[] #output array
for j in name_items:
    m=glob("./"+path+j+"/*")
    for h in m:
        s = cv2.imread(h, cv2.IMREAD_COLOR)       
        s = cv2.resize(s, (100,100))
        s= cv2.cvtColor(s, cv2.COLOR_RGB2BGR)
        extract.append([np.array(s),j])
print(len(extract))
ex=extract

"""Image showing"""

plt.imshow(extract[100][0])

"""Split features and classes"""

x=[]
y=[]
for features,class_value in extract:
    x.append(features)
    y.append(class_value)
x=np.array(x)
y=np.array(y)
x=x.reshape(x.shape[0],100,100,3)
print(x.shape)

"""Make stationary """

x=x/255.0

"""Encoding"""

lb=LabelEncoder()
y=to_categorical(lb.fit_transform(y))
y

"""Starting the session"""

print(x.shape,y.shape)
k1.clear_session()

"""Split into train and test"""

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

y_test.shape

"""Multilayer-perception"""

#mlp
mlp_model=Sequential()
mlp_model.add(Flatten(input_shape=(100,100,3)))
mlp_model.add(Dense(256,activation="relu"))
mlp_model.add(Dropout(0.2))
mlp_model.add(Dense(128,activation="relu"))
mlp_model.add(Dropout(0.2))

mlp_model.add(Dense(5,activation="softmax"))
mlp_model.summary()

"""Compiling"""

mlp_model.compile(loss="categorical_crossentropy",metrics=["accuracy"],optimizer="adam")

"""training data """

history=mlp_model.fit(x_train,y_train,epochs=20,batch_size=128,verbose=1,validation_data=(x_test, y_test))

"""For machine learning classifier like xgboost, random forrest and more, converting the higher dimensions data into lower by using vgg16 model architecture"""

from keras.applications.vgg16 import VGG16
VGG_model = VGG16(weights='imagenet', include_top=False, input_shape=(100,100,3))

"""Make trinable false"""

for layer in VGG_model.layers:
    layer.trainable=False
VGG_model.summary()

feature_extract=VGG_model.predict(x_train)
features=feature_extract.reshape(feature_extract.shape[0],-1)

x_for_training=features
x_for_training.shape

"""XGB model making"""

import xgboost as xgb
xgb_model=xgb.XGBClassifier()
xgb_model.fit(x_for_training,y_train)

"""Converting testing data using vgg16"""

##For test data 
feature_extract_test=VGG_model.predict(x_test)
features_test=feature_extract_test.reshape(feature_extract_test.shape[0],-1)

predicted_xgb=xgb_model.predict(features_test)

print(accuracy_score(y_test,predicted_xgb))

#Random_forrest
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(n_estimators=400)
rf.fit(x_for_training,y_train)

predicted_rf=rf.predict(features_test)
print(accuracy_score(y_test,predicted_rf))

"""Decision tree"""

#Decision tree
from sklearn.tree import DecisionTreeClassifier
tree=DecisionTreeClassifier()
tree.fit(x_for_training,y_train)

predicted_tree=tree.predict(features_test)
print(accuracy_score(y_test,predicted_tree))

x_for_training.shape
y_train.shape

"""Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
svc=SVC(kernel="linear",probability=True)
adaboost_clf=AdaBoostClassifier(base_estimator=svc,n_estimators=50, random_state=0)

adaboost_clf.fit(x_for_training,y_train)

predicted_ada=adaboost_clf.predict(features_test)
print(accuracy_score(y_test,predicted_ada))

"""Stacking Ensemble"""

from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
estimators = [
     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
     ('svr', make_pipeline(StandardScaler(),
                           LinearSVC(random_state=42))) ]

stack_clf = StackingClassifier(
     estimators=estimators, final_estimator=LogisticRegression())

stack_clf.fit(x_for_training,y_train)

predicted_stack=stack_clf.predict(features_test)
print(accuracy_score(y_test,predicted_stack))

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(x_for_training,y_train)

predicted_knn=knn.predict(features_test)
print(accuracy_score(y_test,predicted_knn))

k1.clear_session()

"""Densenet121"""

def ClsModel(n_classes=5, input_shape=(100,100,3)):

    model_d=DenseNet121(weights='imagenet',include_top=False, input_shape=(100, 100, 3)) 


    x=model_d.output


    x= GlobalAveragePooling2D()(x)

    x= BatchNormalization()(x)

    x= Dropout(0.5)(x)

    x= Dense(1024,activation='relu')(x) 

    x= Dense(512,activation='relu')(x) 

    x= BatchNormalization()(x)

    x= Dropout(0.5)(x)


    preds=Dense(n_classes,activation='softmax')(x) #
    model=Model(inputs=model_d.input,outputs=preds)
    return model

model121=ClsModel(5,(100,100,3))
model121.summary()

for layer in model121.layers[:-5]:

    layer.trainable=False

    

for layer in model121.layers[-5:]:

    layer.trainable=True

model121.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])

model121.summary()

history=model121.fit(x_train,y_train,epochs=20,batch_size=128,verbose=1,validation_data=(x_test, y_test))

x_train.shape
k1.clear_session()

"""Densenet169"""

def ClsModel_69(n_classes=5, input_shape=(100,100,3)):
    base_model = DenseNet169(weights=None, include_top=False, input_shape=input_shape)
    x=base_model.output
    x =Flatten()(x)

    x =BatchNormalization()(x)

    

    x =Dropout(0.5)(x)

    x =BatchNormalization()(x)
    x=Dense(1024,activation="relu")(x)
    x =Dense(units=128,
                       activation='relu',
                       )(x)

    x =Dropout(0.5)(x)


    preds=Dense(n_classes,activation='softmax')(x) #
    model=Model(inputs=base_model.input,outputs=preds)
    return model

model169=ClsModel_69(5,(100,100,3))
model169.summary()

for layer in model169.layers[:-5]:

    layer.trainable=False

    

for layer in model169.layers[-5:]:

    layer.trainable=True

model169.compile(loss="categorical_crossentropy",optimizer="adam",metrics=['accuracy'])
model169.summary()

history=model169.fit(x_train,y_train,epochs=20,batch_size=32,verbose=1,validation_data=(x_test, y_test))